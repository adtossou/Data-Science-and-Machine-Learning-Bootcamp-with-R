---
title: 'Machine Learning with R: Linear Regression'
author: "Armand Tossou"
date: "9/6/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Resources

In one of the upcoming lectures, we'll encounter the free ISLR book download link, its been updated, you can now just find the link on the homepage for the latest edition (on the right you see a picture).

Here is that link: www-bcf.usc.edu/~gareth/ISL/

Here is a link to the book directly: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf


## Get our Data

We will use the [Student Performance Data Set from UC Irvine's Machine Learning Repository!](https://archive.ics.uci.edu/ml/datasets/Student+Performance) Download this data our just use the supplied csv files in the notebook repository. We'll specifically look at the math class (student-mat.csv). Make sure to take note that the delimiter is a semi-colon.

```{r}
# check our working directory
getwd()
#  "C:\Users\adtos\Dropbox\Data_Science\R programming\Data Science and Machine Learning Bootcamp with R"

# load the dataset
df <- read.csv('C:/Users/adtos/Dropbox/Data_Science/R programming/Data Science and Machine Learning Bootcamp with R/R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Machine Learning with R/student-mat.csv',sep=';')

```


Preview the data:


```{r}
head(df)

```


Get a summary of the data:


```{r}
summary(df)

```


## Attribute Information
Here is the attribute information for our data set: Attribute Information:

#### Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:

- 1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)
- 2 sex - student's sex (binary: 'F' - female or 'M' - male)
- 3 age - student's age (numeric: from 15 to 22)
- 4 address - student's home address type (binary: 'U' - urban or 'R' - rural)
- 5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)
- 6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)
- 7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
- 8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
- 9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
- 10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
- 11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')
- 12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')
- 13 traveltime - home to school travel time (numeric: 1 - less than 15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - more than 1 hour)
- 14 studytime - weekly study time (numeric: 1 - less than 2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - more than 10 hours)
- 15 failures - number of past class failures (numeric: n if between 1 and 3 , else 4)
- 16 schoolsup - extra educational support (binary: yes or no)
- 17 famsup - family educational support (binary: yes or no)
- 18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
- 19 activities - extra-curricular activities (binary: yes or no)
- 20 nursery - attended nursery school (binary: yes or no)
- 21 higher - wants to take higher education (binary: yes or no)
- 22 internet - Internet access at home (binary: yes or no)
- 23 romantic - with a romantic relationship (binary: yes or no)
- 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
- 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)
- 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)
- 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
- 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
- 29 health - current health status (numeric: from 1 - very bad to 5 - very good)
- 30 absences - number of school absences (numeric: from 0 to 93)

#### These grades are related with the course subject, Math or Portuguese:
- 31 G1 - first period grade (numeric: from 0 to 20)
- 31 G2 - second period grade (numeric: from 0 to 20)
- 32 G3 - final grade (numeric: from 0 to 20, output target)


## Clean the Data

Next we have to clean this data. This data is actually already cleaned for you, But here are some things you may want to consider doing for other data sets:

### Check for NA values

Let's see if we have any NA values:


```{r}
any(is.na(df))

```

Great! Most real data sets will probably have NA or Null values, so its always good to check! Its up to you how to deal with them, either dropping them if they aren't too many, or imputing other values, like the mean value.

### Categorical Features

Moving on, let's make sure that categorical variables have a `factor` set to them. For example, the MJob column refers to categories of Job Types, not some numeric value from 1 to 5. R is actually really good at detecting these sort of values and will take care of this work for you a lot of the time, but always keep in mind the use of `factor()` as a possible. Luckily this is basically already, we can check this using the `str()` function:


```{r}
str(df)

```

## Exploratory Data Analysis

Let's use `ggplot2` to explore the data a bit. Feel free to expand on this section:


```{r}
library(dplyr)
library(ggplot2)
library(ggthemes)

```



### Correlation and CorrPlots

From Wikipedia, correlation is defined as:

- In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or two sets of data. Correlation is any of a broad class of statistical relationships involving dependence, though in common usage it most often refers to the extent to which two variables have a linear relationship with each other. Familiar examples of dependent phenomena include the correlation between the physical statures of parents and their offspring, and the correlation between the demand for a product and its price.

Correlation plots are a great way of exploring data and seeing if there are any interaction terms. Let's start off by just grabbing the numeric data (we can't see correlation for categorical data):


```{r}
# Grab only numeric columns. This is a mask: a vector of booleans
num.cols <- sapply(df, is.numeric)

# Filter to numeric columns for correlation
cor.data <- cor(df[,num.cols])

cor.data

```

While this is fantastic information, it's hard to take it all in. Let's visualize all this data. There are lots of amazing 3rd party packages to do this, let's use and install the `corrgram` package and the `corrplot` package. This will also install a bunch of dependencies for the package.


```{r}
## install packages

#install.packages('corrgram',repos = 'http://cran.us.r-project.org')
#install.packages('corrplot',repos = 'http://cran.us.r-project.org')

# load the packages

library(corrplot)
library(corrgram)

```


Let's start by using corrplot, the most common one. [Here's a really nice documentation page on the package](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html). I encourage you to play around with it.


```{r}
# access the help page for this package
help(corrplot)

```


Create a correlation matrix for all the numeric variables in the dataset:


```{r}
corrplot(cor.data,method='color')

```

Clearly, we have very high correlation between G1, G2, and G3; which makes sense since, those are grades:
- 31 G1 - first period grade (numeric: from 0 to 20)
- 31 G2 - second period grade (numeric: from 0 to 20)
- 32 G3 - final grade (numeric: from 0 to 20, output target)

Meaning good students do well each period, and poor students do poorly each period, etc. Also a high G1,G2, or G3 value has a negative correlation with failure (number of past class failures).

Also Mother and Father education levels are correlated, which also makes sense.

We can also use the `corrgram`, which allows to just automatically do these type of figures by just passing in the dataframe directly. There's a lot going on here, so reference the [documentation of corrgram](https://cran.r-project.org/web/packages/corrgram/index.html) for more info.


```{r}
corrgram(df,order=TRUE, lower.panel=panel.shade,
  upper.panel=panel.pie, text.panel=panel.txt)

```

Since we're going to eventually try to predict the G3 score, let's see a histogram of these scores:


```{r}
ggplot(df,aes(x=G3)) + geom_histogram(bins=20,alpha=0.5,fill='blue') + theme_minimal()

```

Looks like quite a few students get a zero. This is a good place to ask questions, like are students missing the test? Also why is the mean occurrence so high? Is this test curved?

Let's continue by building a model.


## Building a Model

### General Form:

The general model of building a linear regression model in R looks like this:

`model <- lm(y ~ x1 + x2,data)`

or to use all the features in your data

`model <- lm(y ~. , data)` # Uses all features


```{r}


```


### Train and Test Data

We'll need to split our data into a training set and a testing set in order to test our accuracy. We can do this easily using the `caTools` library:

```{r}
# install the package
#install.packages("caTools")

# Import Library
library(caTools)

# Set a random seed so your "random" results are the same as this notebook
set.seed(101) 

# Split up the sample, basically randomly assigns a booleansto a new column "sample"
sample <- sample.split(df$age, SplitRatio = 0.70) # SplitRatio = percent of sample==TRUE

# Training Data
train = subset(df, sample == TRUE)

# Testing Data
test = subset(df, sample == FALSE)

```


### Training our Model

Let's train out model on our training data, then ask for a summary of that model:


```{r}
model <- lm(G3 ~ .,train)

summary(model)

```


### Model Interpretation

Understanding requires a general understanding of statistics, check out Wikipedia for overviews on some of these topics, as well as the ISLR book. Here's a quick guide on understanding the model summary:

#### 1. Residuals:

The residuals are the difference between the actual values of the variable you're predicting and predicted values from your regression--y - ŷ. For most regressions you want your residuals to look like a normal distribution when plotted. If our residuals are normally distributed, this indicates the mean of the difference between our predictions and the actual values is close to 0 (good) and that when we miss, we're missing both short and long of the actual value, and the likelihood of a miss being far from the actual value gets smaller as the distance from the actual value gets larger.

Think of it like a dartboard. A good model is going to hit the bullseye some of the time (but not everytime). When it doesn't hit the bullseye, it's missing in all of the other buckets evenly (i.e. not just missing in the 16 bin) and it also misses closer to the bullseye as opposed to on the outer edges of the dartboard.


#### 2. Significance Stars

The stars are shorthand for significance levels, with the number of asterisks displayed according to the p-value computed. *** for high significance and * for low significance. In this case, *** indicates that it's unlikely that no relationship exists b/w absences and G3 scores.

#### 3. Estimated Coeffecient

The estimated coefficient is the value of slope calculated by the regression. It might seem a little confusing that the Intercept also has a value, but just think of it as a slope that is always multiplied by 1. This number will obviously vary based on the magnitude of the variable you're inputting into the regression, but it's always good to spot check this number to make sure it seems reasonable.

#### 4. Standard Error of the Coefficient Estimate

Measure of the variability in the estimate for the coefficient. Lower means better but this number is relative to the value of the coefficient. As a rule of thumb, you'd like this value to be at least an order of magnitude less than the coefficient estimate.


#### 5. t-value of the Coefficient Estimate

Score that measures whether or not the coefficient for this variable is meaningful for the model. You probably won't use this value itself, but know that it is used to calculate the p-value and the significance levels.

#### 6. Variable p-value

Probability the variable is NOT relevant. You want this number to be as small as possible. If the number is really small, R will display it in scientific notation.

#### 7. Significance Legend

The more punctuation there is next to your variables, the better.

Blank=bad, Dots=pretty good, Stars=good, More Stars=very good

#### 8. Residual Std Error / Degrees of Freedom

The Residual Std Error is just the standard deviation of your residuals. You'd like this number to be proportional to the quantiles of the residuals in #1. For a normal distribution, the 1st and 3rd quantiles should be 1.5 +/- the std error.

The Degrees of Freedom is the difference between the number of observations included in your training sample and the number of variables used in your model (intercept counts as a variable).

#### 9. R-squared

Metric for evaluating the goodness of fit of your model. Higher is better with 1 being the best. Corresponds with the amount of variability in what you're predicting that is explained by the model.
WARNING: While a high R-squared indicates good correlation, [correlation does not always imply causation](http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation).

#### 10. F-statistic & resulting p-value

Performs an [F-test](http://en.wikipedia.org/wiki/F-test) on the model. This takes the parameters of our model (in our case we only have 1) and compares it to a model that has fewer parameters. In theory the model with more parameters should fit better. If the model with more parameters (your model) doesn't perform better than the model with fewer parameters, the F-test will have a high p-value (probability NOT significant boost). If the model with more parameters is better than the model with fewer parameters, you will have a lower p-value.

The DF, or degrees of freedom, pertains to how many variables are in the model. In our case there is one variable so there is one degree of freedom.

Looks like Absences, G1, and G2 scores are good predictors. With age and activities also possibly contributing to a good model.


## Visualize our Model

We can visualize our linear regression model by plotting out the residuals, the residuals are basically a measure of how off we are for each point in the plot versus our model (the error).


```{r}
# Grab residuals
res <- residuals(model)

# Convert to DataFrame for gglpot
res <- as.data.frame(res)

head(res)

```


### Why Plot Residuals?

We want a histogram of our residuals to be normally distributed, something with a strong bimodal distribution may be a warning that our data was not a good fit for lienar regression. However, this can also be hidden from out model. A famous example is [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet).

![Anscombe's quartet](C:/Users/adtos/Dropbox/Data_Science/R programming/Data Science and Machine Learning Bootcamp with R/R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Machine Learning with R//anscombe.png)

### Using ggplot



```{r}
# Histogram of residuals
ggplot(res,aes(res)) +  geom_histogram(fill='blue',alpha=0.5)

```

Looks like there are some suspicious residual values that have a value less than -5. We can further explore this by just calling plot on our model. What these plots represent is outside the course of this lecture, but it's covered in ISLR, as well as the Wikipedia page on [Regression Validation](https://en.wikipedia.org/wiki/Regression_validation).


```{r}
plot(model)

```


Basically after looking at these plots, what you will realize is that our model (behaving as a continuous line, predicted students would get negative scores on their test! Let's make these all zeros when running our results against our predictions.

## Predictions

Let's test our model by predicting on our testing set:

```{r}
G3.predictions <- predict(model,test)

```


Now we can get the root mean squared error (RMSE), a standardized measure of how off we were with our predicted values:


```{r}
results <- cbind(G3.predictions,test$G3) 
colnames(results) <- c('pred','real')
results <- as.data.frame(results)

```


Now let's take care of negative predictions! Lots of ways to this, here's a more complicated way, but its a good example of creating a custom function for a custom problem:

So, let's define a function that converts negative values to zero:

```{r}
to_zero <- function(x){
    if (x < 0){
        return(0)
    }else{
        return(x)
    }
}

```


And let's apply our function to the predictions series:


```{r}
results$pred <- sapply(results$pred,to_zero)

```


There's lots of ways to evaluate the prediction values, for example the MSE (mean squared error):


```{r}
mse <- mean((results$real-results$pred)^2)
print(mse)

```


Or the root mean squared error (RMSE):


```{r}
mse^0.5

```

Or just the R-Squared Value for our model (just for the predictions):


```{r}
SSE = sum((results$pred - results$real)^2)
SST = sum( (mean(df$G3) - results$real)^2)

R2 = 1 - SSE/SST
R2

```


## Conclusion

You should now feel comfortable with the R syntax for a Linear Regression. If some of the plots or math did not make sense to you, make sure to review ISLR and the relevant Wikipedia pages. There is no real substitute for taking the time to read about this material

Up next is an exercise to test your knowledge!

